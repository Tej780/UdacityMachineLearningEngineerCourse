{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the discussion can be found in the previous notebook, so won't be repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"alldata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Development Director\\nALS Therapy Development Institute has an immediate opening for Development Directors. Reporting directly to the Senior Development Director, the Development Director at ALS TDI is a senior fundraising position working to identifying potential prospects and cultivating solicitation strategies and in closing asks with donors including individuals and corporations by building networks via events, generating awareness of ALS TDI; outreach including attending and speaking at events as well as personally cultivates relationships with patients, prospects and donors. This position will be responsible for generating and managing a portfolio of at least two million to five million dollars per year. This position will be located in Atlanta, GA.\\n\\nRequirements:\\nBachelor's Degree requiredMinimum 6-8 years' experience in fundraising and business developmentSuccessful track recording in fundraising with major donors or scientific sales preferredDemonstrated ability to work independently and make progress on several prospects/projects at the same time.Excellent English oral, written and presentation skillsStrong leadership and management skills, as well as the ability to forge strong interpersonal relationshipsAbility to travelExcellent computer skills including use of word processing, spreadsheet, database, presentation and prospect management software operating off a PC-based system including Salesforce.\\nAbout ALS Therapy Development Institute (ALS.net)\\n\\nThe ALS Therapy Development Institute (ALS.net) and its scientists actively discover and develop treatments for ALS. ALS.net is the world's first and largest nonprofit biotech focused 100% on ALS research. Led by ALS patients and their families, the charity understands the urgent need to slow and stop this horrible disease. ALS.net, based in Cambridge, MA, has served as one of the leaders in sharing data and information with academic and ALS research organizations, patients and their families. For more information, visit www.als.net.\\n\\nTo Apply\\nPlease apply at https://als.applicantpro.com. Please include a cover letter, salary requirements and resume. No phone calls please.\\nALS TDI is an equal opportunity employer.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>reviews</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Development Director</td>\n",
       "      <td>ALS TDI</td>\n",
       "      <td>Development Director\\nALS Therapy Development ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An Ostentatiously-Excitable Principal Research...</td>\n",
       "      <td>The Hexagon Lavish</td>\n",
       "      <td>Job Description\\n\\n\"The road that leads to acc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Xpert Staffing</td>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Operation HOPE</td>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Atlanta, GA 30303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assistant Professor -TT - Signal Processing &amp; ...</td>\n",
       "      <td>Emory University</td>\n",
       "      <td>DESCRIPTION\\nThe Emory University Department o...</td>\n",
       "      <td>550.0</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Manager of Data Engineering</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Qualifications\\nBachelor’s degree in Computer ...</td>\n",
       "      <td>385.0</td>\n",
       "      <td>Atlanta, GA 30318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Product Specialist - Periscope, New Ventures</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Qualifications\\nBachelor’s degree\\n5-7 years o...</td>\n",
       "      <td>385.0</td>\n",
       "      <td>Atlanta, GA 30318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Junior to Mid-level Engineer, Geologist or Env...</td>\n",
       "      <td>Wood</td>\n",
       "      <td>Overview / Responsibilities\\nWood Environment ...</td>\n",
       "      <td>899.0</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Analyst - CIB Credit Research</td>\n",
       "      <td>SunTrust</td>\n",
       "      <td>Works closely with senior CIB professionals. P...</td>\n",
       "      <td>3343.0</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Associate - Cognitive Data Scientist Na...</td>\n",
       "      <td>KPMG</td>\n",
       "      <td>Known for being a great place to work and buil...</td>\n",
       "      <td>4494.0</td>\n",
       "      <td>Atlanta, GA 30338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            position             company  \\\n",
       "0                               Development Director             ALS TDI   \n",
       "1  An Ostentatiously-Excitable Principal Research...  The Hexagon Lavish   \n",
       "2                                     Data Scientist      Xpert Staffing   \n",
       "3                                       Data Analyst      Operation HOPE   \n",
       "4  Assistant Professor -TT - Signal Processing & ...    Emory University   \n",
       "5                        Manager of Data Engineering  McKinsey & Company   \n",
       "6       Product Specialist - Periscope, New Ventures  McKinsey & Company   \n",
       "7  Junior to Mid-level Engineer, Geologist or Env...                Wood   \n",
       "8                      Analyst - CIB Credit Research            SunTrust   \n",
       "9  Senior Associate - Cognitive Data Scientist Na...                KPMG   \n",
       "\n",
       "                                         description  reviews  \\\n",
       "0  Development Director\\nALS Therapy Development ...      NaN   \n",
       "1  Job Description\\n\\n\"The road that leads to acc...      NaN   \n",
       "2  Growing company located in the Atlanta, GA are...      NaN   \n",
       "3  DEPARTMENT: Program OperationsPOSITION LOCATIO...     44.0   \n",
       "4  DESCRIPTION\\nThe Emory University Department o...    550.0   \n",
       "5  Qualifications\\nBachelor’s degree in Computer ...    385.0   \n",
       "6  Qualifications\\nBachelor’s degree\\n5-7 years o...    385.0   \n",
       "7  Overview / Responsibilities\\nWood Environment ...    899.0   \n",
       "8  Works closely with senior CIB professionals. P...   3343.0   \n",
       "9  Known for being a great place to work and buil...   4494.0   \n",
       "\n",
       "             location  \n",
       "0  Atlanta, GA 30301   \n",
       "1         Atlanta, GA  \n",
       "2         Atlanta, GA  \n",
       "3  Atlanta, GA 30303   \n",
       "4         Atlanta, GA  \n",
       "5  Atlanta, GA 30318   \n",
       "6  Atlanta, GA 30318   \n",
       "7         Atlanta, GA  \n",
       "8         Atlanta, GA  \n",
       "9   Atlanta, GA 30338  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.position = df.apply(lambda x: str(x.position).lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2381"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~df.position.str.contains(\"machine|learning|data|scientist|engineer|developer\")].position.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.position.str.contains(\"machine|learning|data|scientist||engineer|develop\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {0:\"Machine Learning Engineer\", 1:\"Data Scientist\", 2:\"Data Analyst\", 3:\"Data Engineer\", 4:\"Software Engineer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "reg = [\"(machine|deep).+learning\", \"data.+scientist\", \"data.+analyst\", \"data.+engineer\", \"software.+(engineer|developer)\"]\n",
    "n_classes = len(reg)\n",
    "category = [x for x in range(0,n_classes)]\n",
    "condition = [df.position.str.contains(x) for x in reg]\n",
    "df[\"category\"] = np.select(condition, category, default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df.category>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>reviews</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>Xpert Staffing</td>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data analyst</td>\n",
       "      <td>Operation HOPE</td>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Atlanta, GA 30303</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>assistant professor -tt - signal processing &amp; ...</td>\n",
       "      <td>Emory University</td>\n",
       "      <td>DESCRIPTION\\nThe Emory University Department o...</td>\n",
       "      <td>550.0</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>manager of data engineering</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Qualifications\\nBachelor’s degree in Computer ...</td>\n",
       "      <td>385.0</td>\n",
       "      <td>Atlanta, GA 30318</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>senior associate - cognitive data scientist na...</td>\n",
       "      <td>KPMG</td>\n",
       "      <td>Known for being a great place to work and buil...</td>\n",
       "      <td>4494.0</td>\n",
       "      <td>Atlanta, GA 30338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>senior associate, data scientist</td>\n",
       "      <td>KPMG</td>\n",
       "      <td>Innovate. Collaborate. Shine. Lighthouse — KPM...</td>\n",
       "      <td>4494.0</td>\n",
       "      <td>Atlanta, GA 30338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>business intelligence data science analyst - s...</td>\n",
       "      <td>Newell Brands</td>\n",
       "      <td>Data Science Analyst– Business Intelligence\\nL...</td>\n",
       "      <td>912.0</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>Cotiviti</td>\n",
       "      <td>Cotiviti is looking for an industry leading Da...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>Relus Cloud</td>\n",
       "      <td>DATA SCIENTIST\\n\\nSUMMARY:\\nAs an Amazon Web S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>Inspire Brands</td>\n",
       "      <td>This position is critical to understanding dri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             position             company  \\\n",
       "2                                      data scientist      Xpert Staffing   \n",
       "3                                        data analyst      Operation HOPE   \n",
       "4   assistant professor -tt - signal processing & ...    Emory University   \n",
       "5                         manager of data engineering  McKinsey & Company   \n",
       "9   senior associate - cognitive data scientist na...                KPMG   \n",
       "12                   senior associate, data scientist                KPMG   \n",
       "14  business intelligence data science analyst - s...       Newell Brands   \n",
       "15                                     data scientist            Cotiviti   \n",
       "18                                     data scientist         Relus Cloud   \n",
       "19                                     data scientist      Inspire Brands   \n",
       "\n",
       "                                          description  reviews  \\\n",
       "2   Growing company located in the Atlanta, GA are...      NaN   \n",
       "3   DEPARTMENT: Program OperationsPOSITION LOCATIO...     44.0   \n",
       "4   DESCRIPTION\\nThe Emory University Department o...    550.0   \n",
       "5   Qualifications\\nBachelor’s degree in Computer ...    385.0   \n",
       "9   Known for being a great place to work and buil...   4494.0   \n",
       "12  Innovate. Collaborate. Shine. Lighthouse — KPM...   4494.0   \n",
       "14  Data Science Analyst– Business Intelligence\\nL...    912.0   \n",
       "15  Cotiviti is looking for an industry leading Da...    104.0   \n",
       "18  DATA SCIENTIST\\n\\nSUMMARY:\\nAs an Amazon Web S...      NaN   \n",
       "19  This position is critical to understanding dri...      NaN   \n",
       "\n",
       "              location  category  \n",
       "2          Atlanta, GA         1  \n",
       "3   Atlanta, GA 30303          2  \n",
       "4          Atlanta, GA         0  \n",
       "5   Atlanta, GA 30318          3  \n",
       "9    Atlanta, GA 30338         1  \n",
       "12   Atlanta, GA 30338         1  \n",
       "14         Atlanta, GA         2  \n",
       "15         Atlanta, GA         1  \n",
       "18         Atlanta, GA         1  \n",
       "19         Atlanta, GA         1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for category in data.category.unique():\n",
    "    li.append(data[data.category==category].sample(n=300, replace=True))\n",
    "data = pd.concat(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6393333333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.description.nunique()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    data.description, data.category, test_size=0.2, random_state=1)\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    train_X, train_y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(text):\n",
    "    text = str(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(r\"cache\", \"preprocessed\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, data_val, labels_train, labels_test, labels_val,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [bow(text) for text in data_train]\n",
    "        words_test = [bow(text) for text in data_test]\n",
    "        words_val = [bow(text) for text in data_val]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test, words_val=words_val,\n",
    "                              labels_train=labels_train, labels_test=labels_test, labels_val=labels_val)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, words_val, labels_train, labels_test, labels_val = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['words_val'], cache_data['labels_train'], cache_data['labels_test'], cache_data['labels_val'])\n",
    "    \n",
    "    return words_train, words_test, words_val, labels_train, labels_test, labels_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n",
      "CPU times: user 82.7 ms, sys: 38.4 ms, total: 121 ms\n",
      "Wall time: 91.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preprocess data\n",
    "train_X, test_X, val_X, train_y, test_y, val_y = preprocess_data(train_X, test_X, val_X, train_y, test_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    \n",
    "    word_count = {} \n",
    "    \n",
    "    for review in data:\n",
    "        for word in review:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    sorted_words = [word for word, count in sorted(word_count.items(), key=lambda x:x[1], reverse=True)]\n",
    "\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'data/' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=1000):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=1000):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)\n",
    "val_X, val_X_len = convert_and_pad_data(word_dict, val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_y.reset_index(drop=True), pd.DataFrame(train_X_len, columns=['Length']), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([test_y.reset_index(drop=True), pd.DataFrame(test_X_len, columns=['Length']), pd.DataFrame(test_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([val_y.reset_index(drop=True), pd.DataFrame(val_X_len, columns=['Length']), pd.DataFrame(val_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'val.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size, out_size=\u001b[34m5\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
      "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=out_size)\n",
      "        \n",
      "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[36mNone\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        x = x.t()\n",
      "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\n",
      "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\n",
      "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\n",
      "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\n",
      "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\n",
      "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\n",
      "        \u001b[34mreturn\u001b[39;49;00m out\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM and Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in only the first 250 rows\n",
    "val_sample = pd.read_csv(os.path.join(data_dir, 'val.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "val_sample_y = torch.from_numpy(val_sample[[0]].values).squeeze()\n",
    "val_sample_X = torch.from_numpy(val_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "val_sample_ds = torch.utils.data.TensorDataset(val_sample_X, val_sample_y)\n",
    "# Build the dataloader\n",
    "val_sample_dl = torch.utils.data.DataLoader(val_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use CrossEntropyLoss for the multiclass problem. The model itself has a slightly different structure as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device, valid_loader=None):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # forward propagate\n",
    "            out = model(batch_X)\n",
    "\n",
    "            loss = loss_fn(out, batch_y)\n",
    "\n",
    "            # backwards propagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Loss: {}\".format(total_loss / len(train_loader)))\n",
    "        if valid_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            #val_preds = np.zeros((len(x_cv),len(le.classes_)))\n",
    "\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "                val_loss += loss_fn(y_pred, y_batch).item()\n",
    "                # keep/store predictions\n",
    "                #val_preds[i * batch_size:(i+1) * batch_size] = F.softmax(y_pred).cpu().numpy()\n",
    "\n",
    "            # Check Accuracy\n",
    "            #val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)\n",
    "            print(\"ValLoss: {}\".format(val_loss/len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Classifier Accuracy: 0.18666666666666668\n"
     ]
    }
   ],
   "source": [
    "dum = DummyClassifier(strategy=\"stratified\")\n",
    "dum.fit(train_sample.iloc[:,1:], train_sample.iloc[:,0])\n",
    "print(\"Dummy Classifier Accuracy: {}\".format(dum.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.208\n",
      "Test Accuracy: 0.21333333333333335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13, 10, 11, 22, 17],\n",
       "       [ 2,  2,  3,  3,  4],\n",
       "       [41, 39, 46, 34, 43],\n",
       "       [ 1,  0,  0,  0,  0],\n",
       "       [ 0,  1,  3,  2,  3]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Accuracy:\", accuracy_score(np.array(torch.argmax(model(train_sample_X), dim=1)),np.array(train_sample_y)))\n",
    "test = torch.from_numpy(pd.concat([pd.DataFrame(test_X_len, columns=['Length']), pd.DataFrame(test_X)], axis=1).values).long()\n",
    "print(\"Test Accuracy:\", accuracy_score(np.array(torch.argmax(model(test), dim=1)),np.array(test_y)))\n",
    "confusion_matrix(np.array(torch.argmax(model(test), dim=1)),np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "BCELoss: 1.6039209604263305\n",
      "ValLoss: 1.5786990165710448\n",
      "Epoch: 2\n",
      "BCELoss: 1.3875929832458496\n",
      "ValLoss: 1.552106785774231\n",
      "Epoch: 3\n",
      "BCELoss: 1.0557105541229248\n",
      "ValLoss: 1.6039378643035889\n",
      "Epoch: 4\n",
      "BCELoss: 0.6790406227111816\n",
      "ValLoss: 1.7504199981689452\n",
      "Epoch: 5\n",
      "BCELoss: 0.45246206521987914\n",
      "ValLoss: 1.8674320459365845\n",
      "Epoch: 6\n",
      "BCELoss: 0.3595859229564667\n",
      "ValLoss: 2.058439111709595\n"
     ]
    }
   ],
   "source": [
    "train(model, train_sample_dl, 6, optimizer, loss_fn, device, val_sample_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.912\n",
      "Test Accuracy: 0.3933333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy:\", accuracy_score(np.array(torch.argmax(model(train_sample_X), dim=1)),np.array(train_sample_y)))\n",
    "test = torch.from_numpy(pd.concat([pd.DataFrame(test_X_len, columns=['Length']), pd.DataFrame(test_X)], axis=1).values).long()\n",
    "print(\"Test Accuracy:\", accuracy_score(np.array(torch.argmax(model(test), dim=1)),np.array(test_y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McNemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outcomes = pd.DataFrame({\"Dummy\":dum.predict(test_X)==test_y, \"LSTM\":np.array(torch.argmax(model(test), dim=1))==test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency = pd.crosstab(test_outcomes.Dummy, test_outcomes.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = mcnemar(contingency, exact=False, correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=34.299, p-value=0.000\n"
     ]
    }
   ],
   "source": [
    "print('statistic=%.3f, p-value=%.3f' % (stats.statistic, stats.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistically significant difference, reject Null Hypothesis\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "if stats.pvalue > alpha:\n",
    "    print('Fail to reject Null Hypothesis')\n",
    "else:\n",
    "    print('Statistically significant difference, reject Null Hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the  overall performance is actually worse than the binary case, the result is actually more significant in the multiclass case. The model is much better than random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/JobMulticlassLSTM'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 100,\n",
    "                        'learning_rate': 0.1\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-16 22:07:10 Starting - Starting the training job...\n",
      "2020-06-16 22:07:12 Starting - Launching requested ML instances......\n",
      "2020-06-16 22:08:14 Starting - Preparing the instances for training...\n",
      "2020-06-16 22:09:00 Downloading - Downloading input data...\n",
      "2020-06-16 22:09:24 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,259 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,261 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,275 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,495 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,718 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,718 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,718 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-06-16 22:09:45,719 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b8/7b/01510a6229c2176425bda54d15fba05a4b3df169b87265b008480261d2f9/regex-2020.6.8.tar.gz (690kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, train, regex\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jmn1afbe/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\n",
      "2020-06-16 22:09:44 Training - Training image download completed. Training in progress.\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/9c/e2/cf/246ad8c87bcdf3cba1ec95fa89bc205c9037aa8f4d2e26fdad\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, joblib, regex, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\u001b[0m\n",
      "\u001b[34m    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.9.1 html5lib-1.0.1 joblib-0.14.1 nltk-3.5 numpy-1.18.5 pandas-0.24.2 pytz-2020.1 regex-2020.6.8 soupsieve-2.0.1 tqdm-4.46.1 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.2b1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-06-16 22:10:07,460 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-06-16 22:10:07,474 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"TrainingInputMode\": \"File\"\n",
      "        }\n",
      "    },\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"hyperparameters\": {\n",
      "        \"learning_rate\": 0.1,\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 100\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-2-950783050537/sagemaker-pytorch-2020-06-16-22-07-10-642/source/sourcedir.tar.gz\",\n",
      "    \"log_level\": 20,\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-06-16-22-07-10-642\",\n",
      "    \"current_host\": \"algo-1\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":100,\"learning_rate\":0.1}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"100\",\"--learning_rate\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.1\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-2-950783050537/sagemaker-pytorch-2020-06-16-22-07-10-642/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=100\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":100,\"learning_rate\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-06-16-22-07-10-642\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-2-950783050537/sagemaker-pytorch-2020-06-16-22-07-10-642/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 100 --learning_rate 0.1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 100, vocab_size 5000.\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 1.696179449558258\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 1.7449928522109985\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 1.296511709690094\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 0.9069643616676331\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 0.5998791754245758\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 0.4467068165540695\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 0.38685745000839233\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 0.3538762032985687\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 0.3235075920820236\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 0.3036918342113495\u001b[0m\n",
      "\u001b[34m2020-06-16 22:28:33,564 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-06-16 22:28:45 Uploading - Uploading generated training model\n",
      "2020-06-16 22:28:45 Completed - Training job completed\n",
      "Training seconds: 1185\n",
      "Billable seconds: 1185\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, np.argmax(predictor.predict(array), axis=1))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33,  8,  6,  3,  7],\n",
       "       [12, 18,  4, 13,  5],\n",
       "       [ 3,  6, 43,  5,  6],\n",
       "       [11,  5,  9, 33,  3],\n",
       "       [11,  7,  5,  6, 38]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
